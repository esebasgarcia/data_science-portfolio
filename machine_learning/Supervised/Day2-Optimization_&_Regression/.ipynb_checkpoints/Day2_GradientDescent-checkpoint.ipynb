{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 - Gradient Descent (Part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T16:25:48.262473Z",
     "start_time": "2018-12-03T16:25:47.356221Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T16:25:48.267440Z",
     "start_time": "2018-12-03T16:25:48.264418Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8, 8) #Change matplotlib Box Size\n",
    "plt.rcParams[\"font.size\"] = 14 #Change matplotlib Font Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T16:25:48.567269Z",
     "start_time": "2018-12-03T16:25:48.548809Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Salary_Data.csv')\n",
    "#To Do: Spit the data to X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T16:25:50.299715Z",
     "start_time": "2018-12-03T16:25:50.047226Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'o')\n",
    "plt.xlabel(\"Years of Experience\")\n",
    "plt.ylabel(\"Salary\")\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T16:28:33.935620Z",
     "start_time": "2018-12-03T16:28:33.931056Z"
    }
   },
   "outputs": [],
   "source": [
    "# To Do: Start with a random W and b, between -0.5 and 0.5\n",
    "W = \n",
    "b = \n",
    "print('Initial value of W: %.4f and b: %.4f' % (W, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define total_error over all points in your data\n",
    "\n",
    "2. Initialize b_start, m_start and learning_rate with some values\n",
    "\n",
    "3. Define step_gradient as\n",
    "\n",
    "    a. b_gradient = 0, m_gradient = 0\n",
    "\n",
    "    b. Loop over all your data points and obtain values of \n",
    "       b_gradient and m_gradient as \n",
    "       b_gradient += b_gradient at point i \n",
    "       m_gradient += m_gradient at point i\n",
    "\n",
    "4. Repeat step 3 over N iteration and obtain final value of m and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T16:25:51.636364Z",
     "start_time": "2018-12-03T16:25:51.629896Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    return X*W + b\n",
    "\n",
    "def grad_loss(x , y_true):\n",
    "        # To Do: write the gradient for loss\n",
    "        # get a hint on slide 12!\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "def train(x, y, learning_rate):\n",
    "        global W #in gereral using global variables is not a good thing! Why do we do it here?\n",
    "        global b\n",
    "        grads = grad_loss(x, y)\n",
    "        # To Do: update the weight and bias..\n",
    "        W = \n",
    "        b = \n",
    "    \n",
    "def loss(x, y):\n",
    "    # To Do: the loss function?\n",
    "    \n",
    "    return \n",
    "\n",
    "def accuracy(X, y):\n",
    "    # To Do: compute accuracy for samples X with true values y\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T16:25:54.761171Z",
     "start_time": "2018-12-03T16:25:54.540082Z"
    }
   },
   "outputs": [],
   "source": [
    "#To Do: try different values to see how the model behaves (especially learning rate)\n",
    "learning_rate = 0.01\n",
    "epoch = 100\n",
    "\n",
    "# To Do: Create a random starting W and b\n",
    "W =  \n",
    "b = \n",
    "print('---------------------------------------------')\n",
    "print('Starting value of W: %.4f and b: %.4f' % (W, b))\n",
    "\n",
    "Ws = [W] # List to store W values\n",
    "bs = [b] # List to store b values\n",
    "losses = [accuracy(X, y)] #List to store loss\n",
    "\n",
    "for n in range(epoch):\n",
    "    # To Do: train for each epoch and update parameters\n",
    "    \n",
    "    # Add W to list\n",
    "    # Add b to list\n",
    "    # Add loss to list\n",
    "    print('---------------------------------------------')\n",
    "    print(\"Epoch: \",n+1)\n",
    "    print(\"Learning Rate: \",learning_rate)\n",
    "    print(\"Loss: \",train_acc)\n",
    "    print('Value of W: %.4f and b: %.4f' % (W, b))\n",
    "    \n",
    "print('---------------------------------------------')\n",
    "print('Final value of W: %.4f and b: %.4f after %i epochs: using gradient decent' % (W, b, epoch))\n",
    "print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T17:26:16.274016Z",
     "start_time": "2018-12-03T17:26:16.269485Z"
    }
   },
   "source": [
    "Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T16:25:56.906140Z",
     "start_time": "2018-12-03T16:25:56.734081Z"
    }
   },
   "outputs": [],
   "source": [
    "# To Do: plot the train loss here\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.grid()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T16:25:59.045667Z",
     "start_time": "2018-12-03T16:25:58.885515Z"
    }
   },
   "outputs": [],
   "source": [
    "# To Do: plot the evolution of Ws and bs\n",
    "\n",
    "plt.xlabel('W')\n",
    "plt.ylabel('b')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T16:26:00.085992Z",
     "start_time": "2018-12-03T16:25:59.934551Z"
    }
   },
   "outputs": [],
   "source": [
    "# To Do: plot your fit and compare it to the yestarday's fit\n",
    "\n",
    "plt.xlabel(\"Years of Experience\")\n",
    "plt.ylabel(\"Salary\")\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T16:26:01.964210Z",
     "start_time": "2018-12-03T16:26:01.639949Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle \n",
    "# shuffling points before each iteration, then selecting first N points \n",
    "# This obtains a less biased estimation of the true gradient.\n",
    "# https://www.quora.com/Why-do-we-need-to-shuffle-inputs-for-stochastic-gradient-descent\n",
    "\n",
    "#To Do: try different values to see how the model behaves (especially learning rate)\n",
    "learning_rate = 0.01 \n",
    "batch_size = 10\n",
    "epoch = 100\n",
    "\n",
    "# To Do: Create a random starting W and b\n",
    "W = \n",
    "b = \n",
    "print('Starting value of W: %.4f and b: %.4f' % (W, b))\n",
    "\n",
    "Ws = [W]\n",
    "bs = [b]\n",
    "losses = [accuracy(X, y)]\n",
    "\n",
    "for n in range(epoch):\n",
    "    print('---------------------------------------------')\n",
    "    print(\"Epoch: \",n+1)\n",
    "    print(\"Learning Rate: \",learning_rate)\n",
    "    print(\"Loss: \",train_acc)\n",
    "    # To Do: for each epoch we shuffle the data\n",
    "    \n",
    "    # To Do: train for each epoch and update parameters\n",
    "    \n",
    "    # Add W to list\n",
    "    # Add b to list\n",
    "    # Add loss to list\n",
    "    print('Value of W: %.4f and b: %.4f' % (W, b))\n",
    "\n",
    "print('---------------------------------------------')\n",
    "print('Final value of W: %.4f and b: %.4f after %i epochs: using stochastic gradient decent' % (W, b, epoch))\n",
    "print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T16:26:02.402350Z",
     "start_time": "2018-12-03T16:26:02.213347Z"
    }
   },
   "outputs": [],
   "source": [
    "# To Do: plot the accuracy vs. the epochs\n",
    "\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epochs\");\n",
    "plt.grid()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T16:26:03.212295Z",
     "start_time": "2018-12-03T16:26:03.049673Z"
    }
   },
   "outputs": [],
   "source": [
    "# To Do: the evolution of the weights and bias in each epoch\n",
    "    \n",
    "plt.scatter(Ws, bs, c=range(len(Ws)))\n",
    "plt.xlabel('W')\n",
    "plt.ylabel('b')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T16:26:04.415797Z",
     "start_time": "2018-12-03T16:26:04.261875Z"
    }
   },
   "outputs": [],
   "source": [
    "# To Do: plot the data the gradient descent and the stochastic gradient descent\n",
    "\n",
    "plt.xlabel(\"Years of Experience\")\n",
    "plt.ylabel(\"Salary\")\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Batch (Bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T16:26:06.892754Z",
     "start_time": "2018-12-03T16:26:06.174756Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epoch = 300\n",
    "#To Do: try different values in mini batch size to see how the model behaves\n",
    "mini_batch_size = 20\n",
    "\n",
    "\n",
    "# To Do: work similarly as on gradient descent but break the training once you complete a mini batch size\n",
    "\n",
    "\n",
    "print('---------------------------------------------')\n",
    "print('Final value of W: %.4f and b: %.4f after epochs: %i with mini batches: %i' % (W, b,epoch,mini_batch_size))\n",
    "print('---------------------------------------------')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T16:26:07.417643Z",
     "start_time": "2018-12-03T16:26:07.232897Z"
    }
   },
   "outputs": [],
   "source": [
    "# To Do: plot the accuracy vs. the epochs\n",
    "\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.grid()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T16:26:08.097788Z",
     "start_time": "2018-12-03T16:26:07.901986Z"
    }
   },
   "outputs": [],
   "source": [
    "# To Do: the evolution of the weights and bias in each epoch\n",
    "\n",
    "plt.xlabel('W')\n",
    "plt.ylabel('b')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T16:26:08.626803Z",
     "start_time": "2018-12-03T16:26:08.470361Z"
    }
   },
   "outputs": [],
   "source": [
    "# To Do: compare to previous methods\n",
    "\n",
    "plt.xlabel(\"Years of Experience\")\n",
    "plt.ylabel(\"Salary\")\n",
    "None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
